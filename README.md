# Google AI Open Images – Visual Relationship Track using Deep Learning for Vision and NLP challenge from Kaggle using CNN and LSTM RNN combined model

In this work, we have tried to solve the Visual
Relationship Detection Track competition
launched by Kaggle. The aim of the
competition is to check if computers can
detect the relationship between objects

presented in images. Not only it is a very state-
of-the-art research area, it is a very

challenging task to accomplish compared to
existing computer vision tasks. It is a
combination of two prominent tasks – object
detection and image caption generation.
Although, deep learning models are able to
produce highly accurate results in these
individual tasks, it still struggles to perform
with acceptable accuracy in the visual
relationship detection task. In this paper, we
have attempted a different approach to solve
this problem and compared the result with the
state-of-the-art baseline result. We have

explored two attention-based caption-
generation models from Bengio et.al. (2016) [1]

and Fei-Fei et.al. (2015) [2] and modified
them to solve the visual relation detection task.

[1] K. Xu, J. L. Ba, R. Kiros, K. Cho, A.
Courville, R. Salakhutdinov, R. S.
Zemel and Y. Bengio, “Show, Attend
and Tell: Neural Image Caption
Generation with Visual Attention,” vol.
1502, no. 03044v3, 2016.

[2] J. Johnson, A. Karpathy and L. Fei-Fei,
“DenseCap: Fully convolutional
localization networks for dense
captioning,” vol. 1511, no. 07571v1, 2015
